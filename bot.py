#!/usr/bin/env python3
"""
MTNGanlzrBot ‚Äî Standalone Meeting Analyzer
Audio/Video ‚Üí Transcription ‚Üí Expert Analysis ‚Üí Dynamics
Direct ffmpeg + Deepgram + OpenAI (no external services)

–í–µ—Ä—Å–∏—è —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π:
- –ñ–µ–ª–µ–∑–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ (–Ω–µ –≤—ã–¥—É–º—ã–≤–∞—Ç—å, –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ª–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
- –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –±–µ—Å–µ–¥—ã (–¥–ª—è 2+ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)
- –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤)
"""
import asyncio
import logging
import os
import re
import json
import subprocess
import tempfile
import httpx
from datetime import datetime
from openai import AsyncOpenAI

from telegram import Update, InputFile, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application, CommandHandler, MessageHandler,
    CallbackQueryHandler, filters, ContextTypes
)
from telegram.constants import ChatAction, ParseMode

from pyrogram import Client as PyroClient

from config import config
from report_generator import generate_pdf_report, generate_html_report, safe_filename

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s %(message)s"
)
logger = logging.getLogger("bot")

pyro_client = None
openai_client = None

# ==========================================
# –¢–Å–ü–õ–´–ï –°–û–û–ë–©–ï–ù–ò–Ø
# ==========================================
BOT_MESSAGES = {
    "ru": {
        "file_received": "üí™ –¢–∞—â—É —Ñ–∞–π–ª...",
        "extracting": "üéµ –ò–∑–≤–ª–µ–∫–∞—é –∞—É–¥–∏–æ...",
        "transcribing": "üéß –°–ª—É—à–∞—é –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ...",
        "analyzing": "üß† –ü–æ–≤–µ–∑–ª–æ, —è –∫–∞–∫ —Ä–∞–∑ –≤ —ç—Ç–æ–º –ø—Ä–æ—Ñ–∏...",
        "analyzing_dynamics": "üîÆ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –¥–∏–Ω–∞–º–∏–∫—É –±–µ—Å–µ–¥—ã...",
        "generating": "‚ú® –°–æ–±–∏—Ä–∞—é –º—ã—Å–ª–∏...",
        "done": "üéÅ –í—É–∞–ª—è! –í–∞—à –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤",
        "error": "üòÖ –£–ø—Å, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫",
        "language_prompt": "üåç –ù–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç?",
        "language_selected": "üëç –û—Ç–ª–∏—á–Ω–æ! –ì–æ—Ç–æ–≤–ª—é –æ—Ç—á—ë—Ç",
        "no_speech": "ü§î –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —Ä–µ—á—å",
        "audio_failed": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∞—É–¥–∏–æ",
        "download_failed": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å",
        "yt_downloading": "üì• –°–∫–∞—á–∏–≤–∞—é —Å YouTube...",
        "yt_done": "‚úÖ –°–∫–∞—á–∞–Ω–æ!",
    },
    "en": {
        "file_received": "üí™ Grabbing the file...",
        "extracting": "üéµ Extracting audio...",
        "transcribing": "üéß Listening carefully...",
        "analyzing": "üß† Lucky you, I'm a pro at this...",
        "analyzing_dynamics": "üîÆ Analyzing conversation dynamics...",
        "generating": "‚ú® Gathering my thoughts...",
        "done": "üéÅ Voil√†! Your report is ready",
        "error": "üòÖ Oops, something went wrong",
        "language_prompt": "üåç What language would you like the result in?",
        "language_selected": "üëç Great! Preparing your report",
        "no_speech": "ü§î Couldn't recognize speech",
        "audio_failed": "‚ùå Failed to extract audio",
        "download_failed": "‚ùå Failed to download",
        "yt_downloading": "üì• Downloading from YouTube...",
        "yt_done": "‚úÖ Downloaded!",
    },
    "kk": {
        "file_received": "üí™ –§–∞–π–ª–¥—ã –∞–ª—ã–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
        "extracting": "üéµ –ê—É–¥–∏–æ —à—ã“ì–∞—Ä—ã–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
        "transcribing": "üéß –ú“±“õ–∏—è—Ç —Ç—ã“£–¥–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
        "analyzing": "üß† –°”ô—Ç—Ç—ñ–ª—ñ–∫, –º–µ–Ω –±“±–ª —Å–∞–ª–∞–¥–∞ –º–∞–º–∞–Ω–º—ã–Ω...",
        "analyzing_dynamics": "üîÆ ”ò“£–≥—ñ–º–µ –¥–∏–Ω–∞–º–∏–∫–∞—Å—ã–Ω —Ç–∞–ª–¥–∞—É...",
        "generating": "‚ú® –û–π–ª–∞—Ä—ã–º–¥—ã –∂–∏–Ω–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
        "done": "üéÅ –ú—ñ–Ω–µ! –ï—Å–µ–±—ñ“£—ñ–∑ –¥–∞–π—ã–Ω",
        "error": "üòÖ “ö–∞–ø, –±—ñ—Ä–¥–µ“£–µ –¥“±—Ä—ã—Å –±–æ–ª–º–∞–¥—ã",
        "language_prompt": "üåç –ù”ô—Ç–∏–∂–µ–Ω—ñ “õ–∞–π —Ç—ñ–ª–¥–µ –∞–ª“ì—ã“£—ã–∑ –∫–µ–ª–µ–¥—ñ?",
        "language_selected": "üëç –¢–∞–º–∞—à–∞! –ï—Å–µ–ø –¥–∞–π—ã–Ω–¥–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω",
        "no_speech": "ü§î –°”©–π–ª–µ—É–¥—ñ –∞–Ω—ã“õ—Ç–∞–π –∞–ª–º–∞–¥—ã–º",
        "audio_failed": "‚ùå –ê—É–¥–∏–æ —à—ã“ì–∞—Ä–∞ –∞–ª–º–∞–¥—ã–º",
        "download_failed": "‚ùå –ñ“Ø–∫—Ç–µ–π –∞–ª–º–∞–¥—ã–º",
        "yt_downloading": "üì• YouTube-—Ç–µ–Ω –∂“Ø–∫—Ç–µ–ø –∂–∞—Ç—ã—Ä–º—ã–Ω...",
        "yt_done": "‚úÖ –ñ“Ø–∫—Ç–µ–ª–¥—ñ!",
    },
    "es": {
        "file_received": "üí™ Tomando el archivo...",
        "extracting": "üéµ Extrayendo audio...",
        "transcribing": "üéß Escuchando atentamente...",
        "analyzing": "üß† Qu√© suerte, soy experto en esto...",
        "analyzing_dynamics": "üîÆ Analizando la din√°mica de la conversaci√≥n...",
        "generating": "‚ú® Organizando mis ideas...",
        "done": "üéÅ ¬°Voil√†! Tu informe est√° listo",
        "error": "üòÖ Ups, algo sali√≥ mal",
        "language_prompt": "üåç ¬øEn qu√© idioma quieres el resultado?",
        "language_selected": "üëç ¬°Genial! Preparando tu informe",
        "no_speech": "ü§î No pude reconocer el habla",
        "audio_failed": "‚ùå No se pudo extraer el audio",
        "download_failed": "‚ùå No se pudo descargar",
        "yt_downloading": "üì• Descargando de YouTube...",
        "yt_done": "‚úÖ ¬°Descargado!",
    },
}

# –Ø–∑—ã–∫ ‚Üí –∫–æ–¥ –¥–ª—è Deepgram
LANG_TO_DEEPGRAM = {
    "ru": "ru",
    "en": "en",
    "kk": "kk",
    "es": "es",
    "auto": "ru",  # fallback
}


def get_msg(lang: str, key: str) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ"""
    return BOT_MESSAGES.get(lang, BOT_MESSAGES["ru"]).get(key, BOT_MESSAGES["ru"].get(key, key))


def get_language_keyboard() -> InlineKeyboardMarkup:
    """–°–æ–∑–¥–∞—Ç—å –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞"""
    keyboard = [
        [
            InlineKeyboardButton("üá∑üá∫ –†—É—Å—Å–∫–∏–π", callback_data="lang_ru"),
            InlineKeyboardButton("üá¨üáß English", callback_data="lang_en"),
        ],
        [
            InlineKeyboardButton("üá∞üáø “ö–∞–∑–∞“õ—à–∞", callback_data="lang_kk"),
            InlineKeyboardButton("üá™üá∏ Espa√±ol", callback_data="lang_es"),
        ],
        [
            InlineKeyboardButton("üîÑ –Ø–∑—ã–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞", callback_data="lang_auto"),
        ]
    ]
    return InlineKeyboardMarkup(keyboard)


async def on_startup(app: Application):
    global pyro_client, openai_client
    if config.API_ID and config.API_HASH:
        pyro_client = PyroClient(
            "bot_downloader",
            api_id=config.API_ID,
            api_hash=config.API_HASH,
            bot_token=config.BOT_TOKEN,
            no_updates=True,
            in_memory=True,
        )
        await pyro_client.start()
        logger.info("Pyrogram started (large file support)")
    else:
        logger.warning("No API_ID/API_HASH ‚Äî max 20MB files")
    if config.OPENAI_API_KEY:
        openai_client = AsyncOpenAI(api_key=config.OPENAI_API_KEY)
        logger.info("OpenAI client ready")


async def on_shutdown(app: Application):
    global pyro_client
    if pyro_client:
        await pyro_client.stop()


async def download_file(file_id: str, dest: str, update: Update) -> bool:
    try:
        if pyro_client:
            await pyro_client.download_media(file_id, file_name=dest)
        else:
            f = await update.get_bot().get_file(file_id)
            await f.download_to_drive(dest)
        return True
    except Exception as e:
        logger.error(f"Download failed: {e}")
        return False


def extract_audio(input_path: str):
    output = input_path.rsplit(".", 1)[0] + "_audio.wav"
    try:
        result = subprocess.run(
            ["ffmpeg", "-y", "-i", input_path,
             "-vn", "-acodec", "pcm_s16le",
             "-ar", "16000", "-ac", "1", output],
            capture_output=True, text=True, timeout=300
        )
        if result.returncode == 0 and os.path.exists(output):
            size_mb = os.path.getsize(output) / (1024 * 1024)
            logger.info(f"Audio extracted: {size_mb:.1f} MB")
            return output
        logger.error(f"ffmpeg error: {result.stderr[:500]}")
        return None
    except Exception as e:
        logger.error(f"ffmpeg failed: {e}")
        return None


async def transcribe_audio(audio_path: str, lang: str = "ru"):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π (–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤)"""
    if not config.DEEPGRAM_API_KEY:
        logger.error("No DEEPGRAM_API_KEY!")
        return None, None, 1
    url = "https://api.deepgram.com/v1/listen"
    
    deepgram_lang = LANG_TO_DEEPGRAM.get(lang, "ru")
    if lang == "auto":
        deepgram_lang = "ru"
    
    params = {
        "model": "nova-2",
        "language": deepgram_lang,
        "smart_format": "true",
        "punctuate": "true",
        "paragraphs": "true",
        "diarize": "true",  # ‚Üê –î–ò–ê–†–ò–ó–ê–¶–ò–Ø: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤
        "detect_language": "true" if lang == "auto" else "false",
    }
    try:
        file_size = os.path.getsize(audio_path)
        logger.info(f"Sending {file_size / 1024 / 1024:.1f} MB to Deepgram (with diarization)...")
        async with httpx.AsyncClient(timeout=600.0) as client:
            with open(audio_path, "rb") as f:
                resp = await client.post(
                    url, params=params,
                    headers={
                        "Authorization": f"Token {config.DEEPGRAM_API_KEY}",
                        "Content-Type": "audio/wav",
                    },
                    content=f.read(),
                )
        if resp.status_code != 200:
            logger.error(f"Deepgram {resp.status_code}: {resp.text[:300]}")
            return None, None, 1
        data = resp.json()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
        detected_lang = lang
        if lang == "auto":
            detected = data.get("results", {}).get("channels", [{}])[0].get("detected_language", "ru")
            detected_lang = detected if detected in BOT_MESSAGES else "ru"
        
        channels = data.get("results", {}).get("channels", [])
        if not channels:
            return None, detected_lang, 1
        alternatives = channels[0].get("alternatives", [])
        if not alternatives:
            return None, detected_lang, 1
        
        # –ü–æ–¥—Å—á—ë—Ç —Å–ø–∏–∫–µ—Ä–æ–≤ –∏–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        speakers = set()
        words = alternatives[0].get("words", [])
        for word in words:
            speaker = word.get("speaker")
            if speaker is not None:
                speakers.add(speaker)
        num_speakers = len(speakers) if speakers else 1
        logger.info(f"Detected {num_speakers} speaker(s)")
        
        # –¢–µ–∫—Å—Ç —Å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º–∏
        paragraphs = alternatives[0].get("paragraphs", {})
        if paragraphs and paragraphs.get("paragraphs"):
            parts = []
            for p in paragraphs["paragraphs"]:
                speaker = p.get("speaker", 0)
                speaker_label = f"[–°–ø–∏–∫–µ—Ä {speaker + 1}] " if num_speakers > 1 else ""
                for s in p.get("sentences", []):
                    parts.append(f"{speaker_label}{s.get('text', '')}")
                parts.append("")
            text = "\n".join(parts).strip()
        else:
            text = alternatives[0].get("transcript", "")
        
        logger.info(f"Transcribed: {len(text)} chars, {len(text.split())} words, {num_speakers} speakers")
        return text, detected_lang, num_speakers
    except Exception as e:
        logger.error(f"Deepgram error: {e}")
        return None, None, 1


# ==========================================
# –ü–†–û–ú–ü–¢ –° –ñ–ï–õ–ï–ó–ù–´–ú–ò –ü–†–ê–í–ò–õ–ê–ú–ò
# ==========================================
ANALYSIS_PROMPT_JSON = """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON.

–Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {lang_name}

‚ïê‚ïê‚ïê –ñ–ï–õ–ï–ó–ù–´–ï –ü–†–ê–í–ò–õ–ê ‚ïê‚ïê‚ïê
1. –ù–ò–ß–ï–ì–û –ù–ï –í–´–î–£–ú–´–í–ê–ô. –û–ø–∏—Ä–∞–π—Å—è –¢–û–õ–¨–ö–û –Ω–∞ —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ —Å–∫–∞–∑–∞–Ω–æ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
   –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ –Ω–∞–ø–∏—à–∏. –õ—É—á—à–µ —á–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–±–µ–ª, —á–µ–º –∫—Ä–∞—Å–∏–≤–∞—è –≤—ã–¥—É–º–∫–∞.
2. –ù–µ –ø—Ä–∏–ø–∏—Å—ã–≤–∞–π —É—á–∞—Å—Ç–Ω–∏–∫–∞–º —Å–ª–æ–≤–∞, –º–Ω–µ–Ω–∏—è –∏–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –æ–Ω–∏ –Ω–µ –≤—ã—Ä–∞–∂–∞–ª–∏.
3. –ï—Å–ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫–∏ —Å–æ–≥–ª–∞—Å–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π ‚Äî –æ—Ç—Ä–∞–∑–∏ —ç—Ç–æ. –ù–ï —Å–æ–∑–¥–∞–≤–∞–π –≤–∏–¥–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞,
   —Å–ø–æ—Ä–∞ –∏–ª–∏ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π —Ç–∞–º, –≥–¥–µ –∏—Ö –Ω–µ—Ç. –ï–¥–∏–Ω–æ–¥—É—à–∏–µ ‚Äî —ç—Ç–æ —Ç–æ–∂–µ –≤–∞–∂–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
4. –¶–∏—Ç–∏—Ä—É–π —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –≤—ã—Å–∫–∞–∑—ã–≤–∞–Ω–∏—è –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
5. –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ—è—Å–Ω–æ –∏–ª–∏ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ ‚Äî —É–∫–∞–∂–∏ —ç—Ç–æ –∫–∞–∫ –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å, –∞ –Ω–µ –¥–æ–¥—É–º—ã–≤–∞–π.
6. –ü—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ –ª—É—á—à–µ, —á–µ–º –≤—ã—Å–æ—Å–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–ª—å—Ü–∞ –¥–∞–Ω–Ω—ã–µ.
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON:
{{
  "title": "–ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á–∏",
  "executive_summary": "2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ —Å—É—Ç–∏ –≤—Å—Ç—Ä–µ—á–∏",
  "context": {{
    "industry": "–°—Ñ–µ—Ä–∞/–∏–Ω–¥—É—Å—Ç—Ä–∏—è",
    "meeting_type": "–¢–∏–ø –≤—Å—Ç—Ä–µ—á–∏",
    "complexity": "–ù–∏–∑–∫–∏–π/–°—Ä–µ–¥–Ω–∏–π/–í—ã—Å–æ–∫–∏–π"
  }},
  "goals": {{
    "explicit": ["—è–≤–Ω–∞—è —Ü–µ–ª—å 1", "—è–≤–Ω–∞—è —Ü–µ–ª—å 2"],
    "hidden": ["—Å–∫—Ä—ã—Ç–∞—è —Ü–µ–ª—å 1"]
  }},
  "key_topics": [
    {{"topic": "–¢–µ–º–∞ 1", "details": "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏"}},
    {{"topic": "–¢–µ–º–∞ 2", "details": "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏"}}
  ],
  "positions": {{
    "side_a": {{"label": "–°—Ç–æ—Ä–æ–Ω–∞ –ê", "position": "–ü–æ–∑–∏—Ü–∏—è", "interests": "–ò–Ω—Ç–µ—Ä–µ—Å—ã"}},
    "side_b": {{"label": "–°—Ç–æ—Ä–æ–Ω–∞ –ë", "position": "–ü–æ–∑–∏—Ü–∏—è", "interests": "–ò–Ω—Ç–µ—Ä–µ—Å—ã"}}
  }},
  "agreement_points": ["—Ç–æ—á–∫–∞ —Å–æ–≥–ª–∞—Å–∏—è 1"],
  "disagreement_points": ["—Ç–æ—á–∫–∞ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è 1"],
  "decisions": ["—Ä–µ—à–µ–Ω–∏–µ 1", "—Ä–µ—à–µ–Ω–∏–µ 2"],
  "action_items": [
    {{"task": "–ó–∞–¥–∞—á–∞", "responsible": "–ö—Ç–æ", "deadline": "–ö–æ–≥–¥–∞"}}
  ],
  "swot": {{
    "strengths": ["—Å–∏–ª—å–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞"],
    "weaknesses": ["—Å–ª–∞–±–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ ‚Äî –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å"],
    "opportunities": ["–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å"],
    "threats": ["—É–≥—Ä–æ–∑–∞ ‚Äî –¢–û–õ–¨–ö–û –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∞"]
  }},
  "recommendations": {{
    "substance": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ —Å—É—â–µ—Å—Ç–≤—É"],
    "methodology": ["–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è"]
  }},
  "risks": [
    {{"risk": "–†–∏—Å–∫ ‚Äî –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–π", "severity": "–í—ã—Å–æ–∫–∞—è/–°—Ä–µ–¥–Ω—è—è/–ù–∏–∑–∫–∞—è", "mitigation": "–ö–∞–∫ —Å–Ω–∏–∑–∏—Ç—å"}}
  ],
  "open_questions": ["–≤–æ–ø—Ä–æ—Å 1"],
  "action_plan": {{
    "urgent": ["—Å—Ä–æ—á–Ω–æ 1-7 –¥–Ω–µ–π"],
    "medium": ["—Å—Ä–µ–¥–Ω–µ—Å—Ä–æ–∫ 1-4 –Ω–µ–¥–µ–ª–∏"],
    "long_term": ["–¥–æ–ª–≥–æ—Å—Ä–æ–∫ 1-3 –º–µ—Å—è—Ü–∞"]
  }},
  "kpi": ["KPI 1"],
  "participants_count": 0,
  "conclusion": {{
    "main_insight": "–ì–ª–∞–≤–Ω—ã–π –∏–Ω—Å–∞–π—Ç",
    "key_recommendation": "–ö–ª—é—á–µ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è",
    "forecast": "–ü—Ä–æ–≥–Ω–æ–∑"
  }}
}}

‚ö†Ô∏è –ü–æ–º–Ω–∏: –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–æ—à–ª–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ –∏ –±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ ‚Äî —Ç–∞–∫ –∏ –Ω–∞–ø–∏—à–∏.
–ù–µ –∫–∞–∂–¥–∞—è –≤—Å—Ç—Ä–µ—á–∞ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —Å–ø–∏—Å–∫–µ ¬´—á—Ç–æ —É–ª—É—á—à–∏—Ç—å¬ª.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ markdown, –±–µ–∑ ```json, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."""


# ==========================================
# –ê–ù–ê–õ–ò–ó –î–ò–ù–ê–ú–ò–ö–ò –ë–ï–°–ï–î–´
# ==========================================
DYNAMICS_ANALYSIS_PROMPT = """–¢—ã ‚Äî –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥ –∏ —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –≥—Ä—É–ø–ø–æ–≤–æ–π –¥–∏–Ω–∞–º–∏–∫–µ.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –°–ö–†–´–¢–û–ô –î–ò–ù–ê–ú–ò–ö–ò —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ ‚Äî —Ç–æ, —á—Ç–æ –æ–±—ã—á–Ω–æ
–æ—Å—Ç–∞—ë—Ç—Å—è ¬´–º–µ–∂–¥—É —Å—Ç—Ä–æ–∫¬ª. –≠—Ç–æ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ —Ä–µ—á–µ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö.

–ê—Å–ø–µ–∫—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
1. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –í–õ–ò–Ø–ù–ò–Ø ‚Äî –∫—Ç–æ –∑–∞–¥–∞—ë—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ, –∫—Ç–æ —Å–æ–≥–ª–∞—à–∞–µ—Ç—Å—è, —á—å—ë –º–Ω–µ–Ω–∏–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
2. –ü–ï–†–ï–ë–ò–í–ê–ù–ò–Ø ‚Äî –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∏–ª–∏ –ø–æ–ø—ã—Ç–∫–∏ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞—Ç—å
3. –ú–ê–†–ö–ï–†–´ –ù–ê–ü–†–Ø–ñ–ï–ù–ò–Ø ‚Äî —Ö–µ–¥–∂–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–∞—Å—Å–∏–≤–Ω–∞—è –∞–≥—Ä–µ—Å—Å–∏—è, —É–∫–ª–æ–Ω—á–∏–≤—ã–µ –æ—Ç–≤–µ—Ç—ã
4. –ù–ï–í–´–°–ö–ê–ó–ê–ù–ù–û–ï ‚Äî —Ç–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–º—è–ª–∏ –∏–ª–∏ –∏–∑–±–µ–≥–∞—é—Ç
5. –ö–û–ê–õ–ò–¶–ò–ò ‚Äî –∫—Ç–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —á—å–∏ –∏–¥–µ–∏
6. –≠–ú–û–¶–ò–û–ù–ê–õ–¨–ù–´–ï –°–î–í–ò–ì–ò ‚Äî –º–æ–º–µ–Ω—Ç—ã —Å–º–µ–Ω—ã —Ç–æ–Ω–∞
7. –°–¢–ò–õ–ò –ö–û–ú–ú–£–ù–ò–ö–ê–¶–ò–ò ‚Äî —Ñ–∞–∫—Ç—ã vs —ç–º–æ—Ü–∏–∏, ¬´–º—ã¬ª/¬´—è¬ª/¬´–≤—ã¬ª

‚ïê‚ïê‚ïê –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û ‚ïê‚ïê‚ïê
1. –≠—Ç–æ –ì–ò–ü–û–¢–ï–¢–ò–ß–ï–°–ö–ò–ô –∞–Ω–∞–ª–∏–∑. –§–æ—Ä–º—É–ª–∏—Ä—É–π –∫–∞–∫ ¬´–º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞...¬ª, ¬´–≤–æ–∑–º–æ–∂–Ω–æ...¬ª.
2. –ù–ò–ß–ï–ì–û –ù–ï –í–´–î–£–ú–´–í–ê–ô. –ö–∞–∂–¥–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ ‚Äî —Å —Ü–∏—Ç–∞—Ç–æ–π –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
3. –ó–¥–æ—Ä–æ–≤–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ = —Ç–æ–∂–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ù–µ –∏—â–∏ –ø—Ä–æ–±–ª–µ–º—ã –≥–¥–µ –∏—Ö –Ω–µ—Ç.
4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ü–ê–¢–¢–ï–†–ù–´, –Ω–µ –ª—é–¥–µ–π.
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

–Ø–∑—ã–∫: {language}
–£—á–∞—Å—Ç–Ω–∏–∫–æ–≤: {participants}

JSON:
{{
    "overall_atmosphere": {{
        "summary": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ç–º–æ—Å—Ñ–µ—Ä—ã",
        "tension_level": "low/moderate/elevated/high",
        "collaboration_quality": "high/moderate/low",
        "energy": "energetic/balanced/flat/tense"
    }},
    "power_dynamics": [{{"observation": "", "evidence": "", "confidence": "high/medium/low"}}],
    "tension_markers": [{{"type": "hedging/passive_aggression/evasion", "observation": "", "evidence": "", "confidence": ""}}],
    "healthy_patterns": ["–∑–¥–æ—Ä–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω 1"],
    "key_observations": ["–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ 1 ‚Äî —Ç–æ–ª—å–∫–æ high confidence"]
}}

‚ö†Ô∏è –ï—Å–ª–∏ –∞—Å–ø–µ–∫—Ç –ù–ï –æ–±–Ω–∞—Ä—É–∂–µ–Ω ‚Äî –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ []. –ù–µ –∑–∞–ø–æ–ª–Ω—è–π —Ä–∞–¥–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è.

–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:
{text}"""


LANG_NAMES = {
    "ru": "—Ä—É—Å—Å–∫–∏–π",
    "en": "English",
    "kk": "“õ–∞–∑–∞“õ —Ç—ñ–ª—ñ",
    "es": "espa√±ol",
}


async def analyze_text_json(text: str, lang: str = "ru") -> dict:
    """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ JSON"""
    if not openai_client:
        return None
    max_chars = 100_000
    if len(text) > max_chars:
        text = text[:max_chars] + "\n\n[...—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω...]"
    
    lang_name = LANG_NAMES.get(lang, "—Ä—É—Å—Å–∫–∏–π")
    prompt = ANALYSIS_PROMPT_JSON.format(lang_name=lang_name)
    
    try:
        resp = await openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": f"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:\n\n{text}"},
            ],
            temperature=0.3,
            max_tokens=4000,
        )
        content = resp.choices[0].message.content
        content = content.strip()
        if content.startswith("```"):
            content = content.split("\n", 1)[1] if "\n" in content else content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()
        
        return json.loads(content)
    except json.JSONDecodeError as e:
        logger.error(f"JSON parse error: {e}")
        return None
    except Exception as e:
        logger.error(f"OpenAI error: {e}")
        return None


async def analyze_dynamics(text: str, num_speakers: int, lang: str = "ru") -> dict:
    """–ê–Ω–∞–ª–∏–∑ —Å–∫—Ä—ã—Ç–æ–π –¥–∏–Ω–∞–º–∏–∫–∏ –±–µ—Å–µ–¥—ã (—Ç–æ–ª—å–∫–æ –¥–ª—è 2+ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)"""
    if num_speakers < 2:
        return None
    
    if not openai_client:
        return None
    
    max_chars = 20_000
    if len(text) > max_chars:
        text = text[:max_chars] + "\n\n[...—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω...]"
    
    prompt = DYNAMICS_ANALYSIS_PROMPT.format(
        language=LANG_NAMES.get(lang, "—Ä—É—Å—Å–∫–∏–π"),
        participants=num_speakers,
        text=text
    )
    
    try:
        resp = await openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "–¢—ã ‚Äî –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON. –ï—Å–ª–∏ –¥–∏–Ω–∞–º–∏–∫–∞ –∑–¥–æ—Ä–æ–≤–∞—è ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
            max_tokens=2000,
            response_format={"type": "json_object"},
        )
        content = resp.choices[0].message.content
        return json.loads(content)
    except Exception as e:
        logger.error(f"Dynamics analysis error: {e}")
        return None


def format_dynamics_summary(dynamics: dict, lang: str = "ru") -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ summary –¥–∏–Ω–∞–º–∏–∫–∏ –¥–ª—è —á–∞—Ç–∞"""
    if not dynamics:
        return ""
    
    lines = []
    atm = dynamics.get("overall_atmosphere", {})
    
    # –£—Ä–æ–≤–µ–Ω—å –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è
    tension_map = {
        "low": "üü¢ —Å–ø–æ–∫–æ–π–Ω–∞—è",
        "moderate": "üü° —É–º–µ—Ä–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ", 
        "elevated": "üü† –ø–æ–≤—ã—à–µ–Ω–Ω–æ–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ",
        "high": "üî¥ –≤—ã—Å–æ–∫–æ–µ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ"
    }
    tension = tension_map.get(atm.get("tension_level", ""), "")
    if tension:
        lines.append(f"**–ê—Ç–º–æ—Å—Ñ–µ—Ä–∞:** {tension}")
    
    # Summary
    if atm.get("summary"):
        lines.append(atm["summary"])
    
    # –ö–ª—é—á–µ–≤—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ high confidence)
    key_obs = dynamics.get("key_observations", [])
    if key_obs:
        lines.append("\n**–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:**")
        for obs in key_obs[:3]:
            lines.append(f"  ‚ö° {obs}")
    
    # –ó–¥–æ—Ä–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    healthy = dynamics.get("healthy_patterns", [])
    if healthy:
        lines.append(f"\n**–ó–¥–æ—Ä–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:** {', '.join(healthy[:3])}")
    
    return "\n".join(lines)


def has_notable_dynamics(dynamics: dict) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –µ—Å—Ç—å –ª–∏ —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ –¥–∏–Ω–∞–º–∏–∫–µ"""
    if not dynamics:
        return False
    atm = dynamics.get("overall_atmosphere", {})
    if atm.get("tension_level") in ("n/a", "unknown", None):
        return False
    # –ï—Å—Ç—å —á—Ç–æ-—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ?
    return (
        len(dynamics.get("power_dynamics", [])) > 0 or
        len(dynamics.get("tension_markers", [])) > 0 or
        len(dynamics.get("key_observations", [])) > 0 or
        len(dynamics.get("healthy_patterns", [])) > 0
    )


async def process_content(update: Update, ctx: ContextTypes.DEFAULT_TYPE,
                          file_path: str):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞"""
    chat = update.effective_chat
    lang = ctx.user_data.get("output_language", "ru")
    
    msg = await update.message.reply_text(get_msg(lang, "file_received"))
    audio_path = None
    try:
        await msg.edit_text(get_msg(lang, "extracting"))
        await chat.send_action(ChatAction.TYPING)
        audio_path = await asyncio.get_event_loop().run_in_executor(
            None, extract_audio, file_path
        )
        if not audio_path:
            await msg.edit_text(get_msg(lang, "audio_failed"))
            return

        await msg.edit_text(get_msg(lang, "transcribing"))
        await chat.send_action(ChatAction.TYPING)
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
        text, detected_lang, num_speakers = await transcribe_audio(audio_path, lang)
        
        if lang == "auto" and detected_lang:
            lang = detected_lang
            ctx.user_data["output_language"] = lang
        
        if not text or len(text) < 20:
            await msg.edit_text(get_msg(lang, "no_speech"))
            return

        word_count = len(text.split())
        await msg.edit_text(get_msg(lang, "analyzing"))
        await chat.send_action(ChatAction.TYPING)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        analysis = await analyze_text_json(text, lang)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è 2+ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)
        dynamics = None
        if num_speakers >= 2:
            await msg.edit_text(get_msg(lang, "analyzing_dynamics"))
            dynamics = await analyze_dynamics(text, num_speakers, lang)
        
        if not analysis:
            # Fallback ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
            await msg.edit_text(f"üß† –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω\nüìù –°–ª–æ–≤: {word_count:,}")
            trans_file = tempfile.mktemp(suffix=".txt")
            with open(trans_file, "w", encoding="utf-8") as f:
                f.write(f"=== –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({word_count} —Å–ª–æ–≤) ===\n\n{text}")
            with open(trans_file, "rb") as f:
                await update.message.reply_document(
                    InputFile(f, filename="transcript.txt"),
                    caption=f"üìÑ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({word_count:,} —Å–ª–æ–≤)"
                )
            os.unlink(trans_file)
            return

        await msg.edit_text(get_msg(lang, "generating"))
        await chat.send_action(ChatAction.UPLOAD_DOCUMENT)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        title = analysis.get("title", "–ê–Ω–∞–ª–∏–∑ –≤—Å—Ç—Ä–µ—á–∏")
        date_str = datetime.now().strftime("%Y-%m-%d")
        base_filename = f"{safe_filename(title)}_{date_str}"

        # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç TXT
        trans_file = tempfile.mktemp(suffix=".txt")
        with open(trans_file, "w", encoding="utf-8") as f:
            f.write(f"=== –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({word_count} —Å–ª–æ–≤, {num_speakers} —Å–ø–∏–∫–µ—Ä(–æ–≤)) ===\n\n{text}")

        # 2. PDF –æ—Ç—á—ë—Ç
        pdf_path = generate_pdf_report(analysis, lang)

        # 3. HTML –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
        html_content = generate_html_report(analysis, lang)
        html_path = tempfile.mktemp(suffix=".html")
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        # –§–æ—Ä–º–∏—Ä—É–µ–º summary –¥–ª—è —á–∞—Ç–∞
        summary_lines = [get_msg(lang, "done")]
        summary_lines.append(f"\nüìã **{title}**")
        
        exec_summary = analysis.get("executive_summary", "")
        if exec_summary:
            summary_lines.append(f"\n{exec_summary}")
        
        summary_lines.append(f"\nüìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:** {word_count:,} —Å–ª–æ–≤, {num_speakers} —Å–ø–∏–∫–µ—Ä(–æ–≤)")
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è
        decisions = analysis.get("decisions", [])
        if decisions:
            summary_lines.append("\nüéØ **–†–µ—à–µ–Ω–∏—è:**")
            for d in decisions[:3]:
                summary_lines.append(f"  ‚úÖ {d}")
        
        # –î–∏–Ω–∞–º–∏–∫–∞ –±–µ—Å–µ–¥—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if dynamics and has_notable_dynamics(dynamics):
            dyn_summary = format_dynamics_summary(dynamics, lang)
            if dyn_summary:
                summary_lines.append(f"\nüîÆ **–î–∏–Ω–∞–º–∏–∫–∞ –±–µ—Å–µ–¥—ã:**\n{dyn_summary}")
        
        await msg.edit_text("\n".join(summary_lines), parse_mode=ParseMode.MARKDOWN)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º PDF
        with open(pdf_path, "rb") as f:
            await update.message.reply_document(
                InputFile(f, filename=f"{base_filename}.pdf"),
                caption="üìä –≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –æ—Ç—á—ë—Ç (PDF)"
            )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º HTML
        with open(html_path, "rb") as f:
            await update.message.reply_document(
                InputFile(f, filename=f"{base_filename}.html"),
                caption="üåê –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –æ—Ç—á—ë—Ç (HTML)"
            )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        with open(trans_file, "rb") as f:
            await update.message.reply_document(
                InputFile(f, filename="transcript.txt"),
                caption=f"üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è ({word_count:,} —Å–ª–æ–≤)"
            )

        # Cleanup
        for p in [trans_file, pdf_path, html_path]:
            if p and os.path.exists(p):
                os.unlink(p)

    except Exception as e:
        logger.error(f"Pipeline error: {e}", exc_info=True)
        await msg.edit_text(
            f"{get_msg(lang, 'error')}: <code>{str(e)[:300]}</code>",
            parse_mode=ParseMode.HTML
        )
    finally:
        for p in [file_path, audio_path]:
            if p and os.path.exists(p):
                try:
                    os.unlink(p)
                except Exception:
                    pass


# ==========================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –§–ê–ô–õ–û–í ‚Äî –°–ù–ê–ß–ê–õ–ê –°–ü–†–ê–®–ò–í–ê–ï–ú –Ø–ó–´–ö
# ==========================================

async def save_file_and_ask_language(update: Update, ctx: ContextTypes.DEFAULT_TYPE, 
                                      file_id: str, file_ext: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –∏ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º —è–∑—ã–∫"""
    tmp = tempfile.mktemp(suffix=file_ext)
    if not await download_file(file_id, tmp, update):
        await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª")
        return
    
    ctx.user_data["pending_file"] = tmp
    
    await update.message.reply_text(
        "üåç –ù–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç?",
        reply_markup=get_language_keyboard()
    )


async def handle_language_callback(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ —è–∑—ã–∫–∞"""
    query = update.callback_query
    await query.answer()
    
    lang = query.data.replace("lang_", "")
    ctx.user_data["output_language"] = lang
    
    await query.edit_message_reply_markup(reply_markup=None)
    
    file_path = ctx.user_data.get("pending_file")
    if not file_path or not os.path.exists(file_path):
        await query.message.reply_text("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–Ω–æ–≤–æ")
        return
    
    original_message = ctx.user_data.get("original_message")
    if original_message:
        update._effective_message = original_message
    
    await process_content(update, ctx, file_path)


async def cmd_start(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üß† <b>–¶–∏—Ñ—Ä–æ–≤–æ–π –£–º–Ω–∏–∫</b>\n\n"
        "–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ:\n"
        "üé§ –ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
        "üéµ –ê—É–¥–∏–æ —Ñ–∞–π–ª\n"
        "üé¨ –í–∏–¥–µ–æ —Ñ–∞–π–ª\n"
        "üîó –°—Å—ã–ª–∫—É –Ω–∞ YouTube\n\n"
        "–Ø —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏ —Å–æ–∑–¥–∞–º:\n"
        "üìÑ PDF-–æ—Ç—á—ë—Ç\n"
        "üåê –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π HTML\n"
        "üìù –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é\n"
        "üîÆ –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ (–¥–ª—è 2+ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤)\n\n"
        "–§–∞–π–ª—ã –¥–æ 2 GB.",
        parse_mode=ParseMode.HTML
    )


async def cmd_help(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "üìñ <b>–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:</b>\n\n"
        "1. –û—Ç–ø—Ä–∞–≤—å –∞—É–¥–∏–æ/–≤–∏–¥–µ–æ/–≥–æ–ª–æ—Å–æ–≤–æ–µ\n"
        "2. –í—ã–±–µ—Ä–∏ —è–∑—ã–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n"
        "3. Deepgram —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç (—Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤)\n"
        "4. GPT-4o –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç (—Å –∂–µ–ª–µ–∑–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏)\n"
        "5. –ü–æ–ª—É—á–∞–µ—à—å PDF + HTML + TXT\n\n"
        "üîÆ –î–ª—è –∑–∞–ø–∏—Å–µ–π —Å 2+ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ ‚Äî –∞–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –±–µ—Å–µ–¥—ã\n\n"
        "–§–∞–π–ª—ã –¥–æ 2 GB —á–µ—Ä–µ–∑ Pyrogram.",
        parse_mode=ParseMode.HTML
    )


async def handle_voice(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    voice = update.message.voice or update.message.audio
    if not voice:
        return
    ext = ".ogg" if update.message.voice else ".mp3"
    ctx.user_data["original_message"] = update.message
    await save_file_and_ask_language(update, ctx, voice.file_id, ext)


async def handle_video(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    video = update.message.video or update.message.video_note
    if not video:
        return
    ctx.user_data["original_message"] = update.message
    await save_file_and_ask_language(update, ctx, video.file_id, ".mp4")


async def handle_document(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    doc = update.message.document
    if not doc:
        return
    mime = doc.mime_type or ""
    fname = (doc.file_name or "").lower()
    ok_mime = ("audio", "video", "ogg", "mp4", "mp3", "wav", "m4a",
               "webm", "mpeg", "flac", "aac", "opus")
    ok_ext = (".mp3", ".mp4", ".m4a", ".ogg", ".wav", ".webm", ".flac",
              ".aac", ".mov", ".avi", ".mkv", ".wma", ".opus", ".oga")
    if not (any(t in mime for t in ok_mime) or
            any(fname.endswith(e) for e in ok_ext)):
        await update.message.reply_text("ü§î –û—Ç–ø—Ä–∞–≤—å –∞—É–¥–∏–æ –∏–ª–∏ –≤–∏–¥–µ–æ —Ñ–∞–π–ª.")
        return
    size_mb = (doc.file_size or 0) / (1024 * 1024)
    if size_mb > config.MAX_FILE_MB:
        await update.message.reply_text(f"üì¶ –ú–∞–∫—Å. {config.MAX_FILE_MB} MB.")
        return
    logger.info(f"Downloading: {doc.file_name} ({size_mb:.1f} MB)")
    ext = os.path.splitext(doc.file_name or "file.mp4")[1] or ".mp4"
    ctx.user_data["original_message"] = update.message
    await save_file_and_ask_language(update, ctx, doc.file_id, ext)


async def handle_text(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    text = (update.message.text or "").strip()
    if not text:
        return
    yt = re.search(r'(https?://(www\.)?(youtube\.com|youtu\.be)/\S+)', text)
    if yt:
        url = yt.group(1)
        msg = await update.message.reply_text("üì• –°–∫–∞—á–∏–≤–∞—é —Å YouTube...")
        try:
            tmp = tempfile.mktemp(suffix=".m4a")
            r = subprocess.run(
                ["yt-dlp", "-x", "--audio-format", "m4a", "-o", tmp, url],
                capture_output=True, text=True, timeout=600
            )
            if r.returncode == 0 and os.path.exists(tmp):
                await msg.edit_text("‚úÖ –°–∫–∞—á–∞–Ω–æ!")
                ctx.user_data["pending_file"] = tmp
                ctx.user_data["original_message"] = update.message
                await update.message.reply_text(
                    "üåç –ù–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ —Ö–æ—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç?",
                    reply_markup=get_language_keyboard()
                )
            else:
                await msg.edit_text(f"‚ùå –û—à–∏–±–∫–∞ YouTube:\n<code>{r.stderr[:200]}</code>",
                                    parse_mode=ParseMode.HTML)
        except Exception as e:
            await msg.edit_text(f"‚ùå {str(e)[:200]}")
        return
    await update.message.reply_text(
        "ü§î –û—Ç–ø—Ä–∞–≤—å –∞—É–¥–∏–æ, –≤–∏–¥–µ–æ, –≥–æ–ª–æ—Å–æ–≤–æ–µ –∏–ª–∏ —Å—Å—ã–ª–∫—É –Ω–∞ YouTube."
    )


async def error_handler(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    logger.error(f"Error: {ctx.error}", exc_info=ctx.error)


def main():
    if not config.BOT_TOKEN:
        logger.error("TELEGRAM_BOT_TOKEN not set!")
        return
    app = Application.builder().token(config.BOT_TOKEN).build()
    app.post_init = on_startup
    app.post_shutdown = on_shutdown
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CallbackQueryHandler(handle_language_callback, pattern="^lang_"))
    app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO, handle_voice))
    app.add_handler(MessageHandler(filters.VIDEO | filters.VIDEO_NOTE, handle_video))
    app.add_handler(MessageHandler(filters.Document.ALL, handle_document))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    app.add_error_handler(error_handler)
    logger.info("Bot started!")
    logger.info(f"  Pyrogram: {'yes' if config.API_ID else 'no'}")
    logger.info(f"  Deepgram: {'yes' if config.DEEPGRAM_API_KEY else 'NO!'}")
    logger.info(f"  OpenAI: {'yes' if config.OPENAI_API_KEY else 'NO!'}")
    logger.info("  Features: Iron Rules ‚úì, Diarization ‚úì, Dynamics Analysis ‚úì")
    app.run_polling(drop_pending_updates=True)


if __name__ == "__main__":
    main()
v4: Iron Rules + Diarization + Dynamics Analysis
